import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { createClient } from '@supabase/supabase-js';
import { logUsage } from '@/lib/usage';

export const runtime = 'nodejs';
export const dynamic = 'force-dynamic';
export const maxDuration = 300; // 5 minutes for large documents

// Maximum file size: 18MB (same as client limit)
const MAX_FILE_SIZE = 18 * 1024 * 1024;
const MAX_IMAGES = 50; // Maximum number of pages/images to process
const MAX_TOKENS_PER_REQUEST = 4096; // Leave room for input tokens (context is 8192 total)
const PAGES_PER_BATCH = 5; // Process 5 pages at a time to avoid context limits

/**
 * DeepSeek OCR Configuration:
 * - Compression Ratio: 9x (configured via 'high' detail level)
 * - Accuracy: 97% OCR precision at 9x compression
 * - Vision Tokens: ~256-800 tokens per page at high resolution
 * - Context Window: 8192 tokens total (input + output)
 *
 * At 9x compression, text that would take 9 tokens is represented by 1 vision token.
 * A 1024Ã—1024 image uses ~256 vision tokens after compression.
 * The scale 2.5 rendering creates larger images for better OCR accuracy.
 */

// Initialize DeepInfra client with OpenAI SDK
const deepinfra = new OpenAI({
  baseURL: process.env.DEEPINFRA_BASE_URL || 'https://api.deepinfra.com/v1/openai',
  apiKey: process.env.DEEPINFRA_API_KEY || '',
});

// Initialize Supabase client for usage logging
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

/**
 * Convert an image file to base64 data URL
 */
async function imageToBase64(file: File): Promise<string> {
  const buffer = await file.arrayBuffer();
  const base64 = Buffer.from(buffer).toString('base64');
  const mimeType = file.type || 'image/jpeg';
  return `data:${mimeType};base64,${base64}`;
}

/**
 * Process a batch of images with DeepSeek OCR
 * Uses 9x compression ratio for 97% accuracy
 */
async function processBatchWithDeepSeekOCR(
  images: string[],
  startIndex: number,
  totalImages: number
): Promise<{ text: string; inputTokens: number; outputTokens: number }> {
  let batchText = '';
  let batchInputTokens = 0;
  let batchOutputTokens = 0;

  console.log(`[deepseek-ocr] Processing batch: images ${startIndex + 1}-${startIndex + images.length}`);

  for (let i = 0; i < images.length; i++) {
    const globalIndex = startIndex + i;
    console.log(`[deepseek-ocr] Processing page ${globalIndex + 1}/${totalImages}`);

    try {
      const completion = await deepinfra.chat.completions.create({
        model: 'deepseek-ai/DeepSeek-OCR',
        messages: [
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: '<image>\n<|grounding|>Convert the document to markdown. Extract all text, headings, lists, tables, and formulas. Preserve the structure and formatting.',
              },
              {
                type: 'image_url',
                image_url: {
                  url: images[i],
                  // High detail for 9x compression ratio (97% accuracy)
                  detail: 'high',
                },
              },
            ],
          },
        ],
        temperature: 0.0, // Deterministic for OCR accuracy
        max_tokens: MAX_TOKENS_PER_REQUEST, // 4096 to leave room for input
      });

      const pageText = completion.choices[0]?.message?.content || '';

      // Only add page headers for multi-page documents
      if (totalImages > 1) {
        batchText += `## Page ${globalIndex + 1}\n\n${pageText}\n\n---\n\n`;
      } else {
        batchText += pageText;
      }

      // Track token usage
      const inputTokens = completion.usage?.prompt_tokens || 0;
      const outputTokens = completion.usage?.completion_tokens || 0;
      batchInputTokens += inputTokens;
      batchOutputTokens += outputTokens;

      console.log(
        `[deepseek-ocr] Page ${globalIndex + 1} complete: ${pageText.length} chars, ${inputTokens} input tokens, ${outputTokens} output tokens`
      );
    } catch (error) {
      console.error(`[deepseek-ocr] Error processing page ${globalIndex + 1}:`, error);
      throw error;
    }
  }

  return { text: batchText, inputTokens: batchInputTokens, outputTokens: batchOutputTokens };
}

/**
 * Process images with DeepSeek OCR in batches
 * Batching prevents exceeding context window limits for large documents
 */
async function processWithDeepSeekOCR(
  images: string[],
  userId: string | null,
  ip: string | null
): Promise<string> {
  let fullText = '';
  let totalInputTokens = 0;
  let totalOutputTokens = 0;

  console.log(`[deepseek-ocr] Processing ${images.length} images in batches of ${PAGES_PER_BATCH}`);

  // Process images in batches to avoid context window limits
  for (let i = 0; i < images.length; i += PAGES_PER_BATCH) {
    const batch = images.slice(i, Math.min(i + PAGES_PER_BATCH, images.length));
    const batchNumber = Math.floor(i / PAGES_PER_BATCH) + 1;
    const totalBatches = Math.ceil(images.length / PAGES_PER_BATCH);

    console.log(`[deepseek-ocr] Processing batch ${batchNumber}/${totalBatches} (${batch.length} pages)`);

    try {
      const { text, inputTokens, outputTokens } = await processBatchWithDeepSeekOCR(batch, i, images.length);
      fullText += text;
      totalInputTokens += inputTokens;
      totalOutputTokens += outputTokens;

      console.log(
        `[deepseek-ocr] Batch ${batchNumber}/${totalBatches} complete: ${inputTokens} input tokens, ${outputTokens} output tokens`
      );
    } catch (error) {
      console.error(`[deepseek-ocr] Error processing batch ${batchNumber}:`, error);
      throw error;
    }
  }

  // Log usage to Supabase
  if (userId || ip) {
    try {
      await logUsage(
        supabase,
        userId,
        ip,
        'deepseek-ai/DeepSeek-OCR',
        {
          input_tokens: totalInputTokens,
          output_tokens: totalOutputTokens,
        },
        {
          metadata: {
            route: '/api/upload/parse',
            provider: 'deepinfra',
            num_pages: images.length,
          },
        }
      );
      console.log('[deepseek-ocr] Usage logged successfully');
    } catch (logError) {
      console.error('[deepseek-ocr] Error logging usage:', logError);
      console.error('[deepseek-ocr] Log error stack:', logError instanceof Error ? logError.stack : 'No stack');
      // Don't fail the request if logging fails
    }
  }

  console.log(
    `[deepseek-ocr] Complete! Total: ${totalInputTokens} input tokens, ${totalOutputTokens} output tokens, ${fullText.length} characters extracted`
  );

  console.log('[deepseek-ocr] Returning text, first 100 chars:', fullText.substring(0, 100));
  return fullText;
}

export async function POST(request: NextRequest) {
  try {
    console.log('[deepseek-ocr] POST request received');

    // Get user ID from Supabase session
    const authCookie = request.cookies.get('sb-wdoxbjhsiakjzebabpwm-auth-token');
    let userId: string | null = null;

    if (authCookie) {
      try {
        const { data: { user } } = await supabase.auth.getUser(authCookie.value);
        userId = user?.id || null;
        console.log('[deepseek-ocr] User authenticated:', userId ? 'yes' : 'no');
      } catch (authError) {
        console.log('[deepseek-ocr] Could not get user from session:', authError instanceof Error ? authError.message : 'Unknown error');
      }
    } else {
      console.log('[deepseek-ocr] No auth cookie found');
    }

    const ip = request.headers.get('x-forwarded-for') ||
               request.headers.get('x-real-ip') ||
               null;

    console.log('[deepseek-ocr] Client IP:', ip || 'unknown');

    const contentType = request.headers.get('content-type') || '';
    console.log('[deepseek-ocr] Content-Type:', contentType);

    // Handle two formats: FormData (with file) or JSON (with base64 images array)
    let images: string[] = [];
    let fileName = 'document';
    let fileSize = 0;

    if (contentType.includes('application/json')) {
      console.log('[deepseek-ocr] Processing JSON request');
      // JSON format: array of base64 images (for PDF pages converted client-side)
      const body = await request.json();
      images = body.images || [];
      fileName = body.fileName || 'document';
      fileSize = body.fileSize || 0;

      if (!Array.isArray(images) || images.length === 0) {
        return NextResponse.json(
          { error: 'No images provided in request body' },
          { status: 400 }
        );
      }

      if (images.length > MAX_IMAGES) {
        return NextResponse.json(
          { error: `Too many images. Maximum ${MAX_IMAGES} pages supported.` },
          { status: 400 }
        );
      }

      console.log('[deepseek-ocr] JSON request parsed:', images.length, 'images');
    } else {
      console.log('[deepseek-ocr] Processing FormData request');

      // FormData format: single image file
      const formData = await request.formData();
      const file = formData.get('file') as File;

      if (!file) {
        return NextResponse.json(
          { error: 'No file provided' },
          { status: 400 }
        );
      }

      // Validate file size
      if (file.size > MAX_FILE_SIZE) {
        return NextResponse.json(
          { error: `File size exceeds ${MAX_FILE_SIZE / (1024 * 1024)}MB limit` },
          { status: 400 }
        );
      }

      fileName = file.name;
      fileSize = file.size;

      const fileType = file.type.toLowerCase();
      const fileNameLower = file.name.toLowerCase();

      // Only support image files via FormData
      if (
        fileType.startsWith('image/') ||
        fileNameLower.match(/\.(png|jpg|jpeg|webp|gif|bmp)$/i)
      ) {
        const base64Image = await imageToBase64(file);
        images = [base64Image];
      } else {
        return NextResponse.json(
          {
            error: `Unsupported file format: ${fileType}. Please upload an image file or send PDF pages as base64 images via JSON.`,
          },
          { status: 400 }
        );
      }
    }

    console.log('[deepseek-ocr] Parsing:', fileName, 'Size:', fileSize, 'bytes', 'Images:', images.length);

    // Process with DeepSeek OCR
    console.log('[deepseek-ocr] Starting OCR processing...');
    const extractedText = await processWithDeepSeekOCR(images, userId, ip);
    console.log('[deepseek-ocr] OCR processing complete, got text:', extractedText ? 'yes' : 'no', 'length:', extractedText?.length || 0);

    if (!extractedText || extractedText.trim().length === 0) {
      return NextResponse.json(
        { error: 'No text could be extracted from the document' },
        { status: 500 }
      );
    }

    console.log(
      '[deepseek-ocr] Successfully parsed:',
      fileName,
      'Extracted:',
      extractedText.length,
      'characters from',
      images.length,
      'pages'
    );

    // Sanitize extracted text to remove any problematic characters
    // that might cause JSON serialization issues
    let sanitizedText = extractedText;
    try {
      // Remove null bytes and other control characters that might break JSON
      sanitizedText = extractedText.replace(/\0/g, '').replace(/[\x00-\x08\x0B\x0C\x0E-\x1F]/g, '');
      console.log('[deepseek-ocr] Text sanitized, length after sanitization:', sanitizedText.length);
    } catch (sanitizeError) {
      console.error('[deepseek-ocr] Error sanitizing text:', sanitizeError);
      // Continue with original text if sanitization fails
      sanitizedText = extractedText;
    }

    // Prepare response
    const responseData = {
      success: true,
      text: sanitizedText,
      fileName: fileName,
      fileSize: fileSize,
      numPages: images.length,
    };

    console.log('[deepseek-ocr] Preparing response:', {
      fileName: responseData.fileName,
      textLength: responseData.text.length,
      numPages: responseData.numPages,
    });

    try {
      // Test JSON serialization first
      const testJson = JSON.stringify(responseData);
      console.log('[deepseek-ocr] JSON serialization test successful, size:', testJson.length, 'bytes');
      console.log('[deepseek-ocr] First 200 chars of JSON:', testJson.substring(0, 200));

      // Create response with explicit headers
      const response = NextResponse.json(responseData, {
        status: 200,
        headers: {
          'Content-Type': 'application/json',
        },
      });

      console.log('[deepseek-ocr] NextResponse created successfully');
      console.log('[deepseek-ocr] Response status:', response.status);
      console.log('[deepseek-ocr] Response headers:', Object.fromEntries(response.headers.entries()));

      return response;
    } catch (serializationError) {
      console.error('[deepseek-ocr] JSON serialization error:', serializationError);
      console.error('[deepseek-ocr] Error details:', serializationError instanceof Error ? serializationError.stack : 'No stack');

      // Return a safe error response
      return NextResponse.json(
        {
          error: 'Failed to serialize response',
          details: serializationError instanceof Error ? serializationError.message : 'Unknown error'
        },
        { status: 500 }
      );
    }
  } catch (error) {
    console.error('[deepseek-ocr] Error parsing file:', error);
    console.error('[deepseek-ocr] Error stack:', error instanceof Error ? error.stack : 'No stack trace');

    const errorMessage =
      error instanceof Error ? error.message : 'Failed to parse document';

    return NextResponse.json({ error: errorMessage }, { status: 500 });
  }
}
